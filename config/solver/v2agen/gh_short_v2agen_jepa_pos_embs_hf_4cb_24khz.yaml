# @package __global__

# This is a minimal debugging configuration
# for V2AGen training solver
defaults:
  - v2agen/default
  - /model: lm/v2agen_lm
  - override /model/lm/model_scale: small
  - override /dset: video/gh
  - _self_

autocast: true
autocast_dtype: float16

compression_model_checkpoint: facebook/encodec_24kHz

channels: 1
sample_rate: 24000

conditioners:
  video:
    jepa:
      use_masking: false
      sample_rate: 25
      max_frames: 16
      autocast_dtype: ${autocast_dtype}
      use_pos_embed: true

deadlock:
  use: true  # deadlock detection

transformer_lm:
  n_q: 4

codebooks_pattern:
  modeling: delay
  delay:
    delays: [0, 1, 2, 3]
    flatten_first: 0
    empty_initial: 0

dataset:
  num_workers: 16
  batch_size: 48 # 2 gpus
  segment_duration: 0.64
  sample_on_weight: false  # Uniform sampling all the way
  sample_on_duration: false  # Uniform sampling all the way
  clip_video: true
  frames_per_clip: ${conditioners.video.jepa.patch_size}
  frame_step: 1
  train:
    num_samples: 23389
    rand_transform_prob: 0.5
    video_transforms:
      - target: torchvision.transforms.v2.Resize
        params:
          size: 256
          antialias: true
      - target: torchvision.transforms.v2.RandomCrop
        params:
          size: [224, 224]
      - target: torchvision.transforms.v2.RandomHorizontalFlip
        params:
          p: ${dataset.train.rand_transform_prob}
      - target: audiocraft.utils.transforms.ToFloat32DType
      - target: torchvision.transforms.v2.Normalize
        params:
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]
      - target: audiocraft.utils.transforms.Permute
        params:
          permutation: [1, 0, 2, 3]
  valid:
    num_samples: 2324
    video_transforms:
      - target: torchvision.transforms.v2.Resize
        params:
          size: 256
          antialias: true
      - target: torchvision.transforms.v2.CenterCrop
        params:
          size: [224, 224]
      - target: audiocraft.utils.transforms.ToFloat32DType
      - target: torchvision.transforms.v2.Normalize
        params:
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]
      - target: audiocraft.utils.transforms.Permute
        params:
          permutation: [1, 0, 2, 3]
  evaluate:
    num_samples: 8452
    video_transforms:
      - target: torchvision.transforms.v2.Resize
        params:
          size: 256
          antialias: true
      - target: torchvision.transforms.v2.CenterCrop
        params:
          size: [224, 224]
      - target: audiocraft.utils.transforms.ToFloat32DType
      - target: torchvision.transforms.v2.Normalize
        params:
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]
      - target: audiocraft.utils.transforms.Permute
        params:
          permutation: [1, 0, 2, 3]
  generate:
    num_samples: 8452
    video_transforms:
      - target: torchvision.transforms.v2.Resize
        params:
          size: 256
          antialias: true
      - target: torchvision.transforms.v2.CenterCrop
        params:
          size: [224, 224]
      - target: audiocraft.utils.transforms.ToFloat32DType
      - target: torchvision.transforms.v2.Normalize
        params:
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]
      - target: audiocraft.utils.transforms.Permute
        params:
          permutation: [1, 0, 2, 3]

generate:
  every: 10
  num_workers: 10
  audio:
    log_clipping: false
  lm:
    prompted_samples: false
    unprompted_samples: true
    use_sampling: true
    top_k: 250
    top_p: 0.0
evaluate:
  every: null

optim:
  epochs: 60
  updates_per_epoch: 480
  optimizer: adamw
  lr: 0.000154 # exp(-0.4214*ln(N)-0.5535)
  adam:
    weight_decay: 1.54e-05 # lr * 0.1

logging:
  log_tensorboard: true
  log_updates: 48
tensorboard:
  with_media_logging: true

schedule:
  lr_scheduler: cosine
  cosine:
    warmup: 200
    lr_min_ratio: 0.1
    cycle_length: 1.0
